use whitehole::lexer::{
  action::{exact, regex, simple, whitespaces},
  token::{token_kind, Range, Token},
  LexerBuilder,
};

// define token kinds, make sure it is decorated by `#[token_kind]`
#[token_kind]
#[derive(Clone, Default)]
enum MyKind {
  #[default]
  Anonymous,
  A,
  BC,
}

#[test]
fn lexer_basics() {
  // the text to be lexed
  let text = "a b c";

  // create a lexer via the lexer builder
  let mut lexer = LexerBuilder::new()
    // a lexer consists of many actions which will digest some bytes from the input string.
    // you can use `ignore` to define muted actions which will be accepted during the lexing process,
    // digest some bytes from the input string, but no token will be emitted.
    // for example, you may want to digest all whitespaces but not emit any token for them
    .ignore(
      // we have many built-in action utils so you don't need to write your actions from scratch,
      // e.g. you can use `whitespaces` to match all unicode whitespaces
      // and use `bind` to bind the action to a token kind.
      // be ware: the `Anonymous` is NOT the variant of the `MyKind` enum,
      // it is a struct generated by the `#[token_kind]` macro.
      whitespaces().bind(Anonymous),
    )
    // if you want to bind the action to the default token kind
    // you can also use `ignore_default` for short
    .ignore_default(whitespaces())
    // you can use `define` to define actions which will emit tokens,
    // the first parameter is the target token kind,
    // the second parameter is the action
    .define(
      // the target token kind
      A,
      // we use the built-in action `exact` to match the exact string "a"
      // we don't need to call `bind` here because the action will be bound to `A`
      exact("a"),
    )
    .define(
      BC,
      // you can also provide multiple actions in one `define` call,
      // these actions will have the same target token kind
      [
        // you can create an action from a regex pattern,
        // remember to use `^` to match from the start of the rest string
        regex(r"^b"),
        // you can also write your own action by using `simple`,
        // the closure will receive the input and return the number of bytes to be consumed,
        // if the closure returns 0, the action will be considered as rejected
        simple(|input| if input.rest().starts_with("c") { 1 } else { 0 }),
      ],
    )
    // load the input string
    .build(text);

  // now let's try to lex the input string,
  // the first token should be `a`
  let token = lexer.lex().token.unwrap();
  assert!(matches!(
      token,
      Token {
        range: Range { start: 0, end: 1 },
        error: None,
        kind
      } if matches!(kind.value(), MyKind::A)
  ));
  // we don't store the token's content in the token itself,
  // you can get the content by using the token's range
  assert_eq!(&text[token.range], "a");

  // because whitespaces are muted and ignored,
  // no token will be emitted for it.
  // the second token should be `b`
  let token = lexer.lex().token.unwrap();
  assert!(matches!(
      token,
      Token {
        range: Range { start: 2, end: 3 },
        error: None,
        kind
      } if matches!(kind.value(), MyKind::BC)
  ));
  assert_eq!(&text[token.range], "b");

  // the third token should be `c`
  let token = lexer.lex().token.unwrap();
  assert!(matches!(
      token,
      Token {
        range: Range { start: 4, end: 5 },
        error: None,
        kind
      } if matches!(kind.value(), MyKind::BC)
  ));
  assert_eq!(&text[token.range], "c");
}
